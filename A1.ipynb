{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dlkt101101/CS431/blob/main/A1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLRqn3y2NfJi"
      },
      "source": [
        "## CS431/631 Big Data Infrastructure\n",
        "### Winter 2025 - Assignment 1\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2dtUlLTNfJl"
      },
      "source": [
        "**Please edit this (text) cell to provide your name and UW student ID number!**\n",
        "* **Name:** _replace this with your name_\n",
        "* **ID:** _replace this with your UW student ID number_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3DCzT-rNfJm"
      },
      "source": [
        "---\n",
        "#### Overview\n",
        "For this assignment, you will be using Python to analyze the [pointwise mutual information (PMI)](http://en.wikipedia.org/wiki/Pointwise_mutual_information) of tokens in the text of Shakespeare's plays.    For this assignment, you will need the same text file (`Shakespeare.txt`) that you used for Assignment 0.   You will also need the Python tokenizer module, `simple_tokenize.py`.\n",
        "\n",
        "If two events $x$ and $y$ are independent, their PMI will be zero.   A positive PMI indicates that $x$ and $y$ are more likely to co-occur than they would be if they were independent.   Similarly, a negative PMI indicates that $x$ and $y$ are less likely to co-occur.   The PMI of events $x$ and $y$ is given by\n",
        "\\begin{equation*}\n",
        "PMI(x,y) = \\log\\frac{p(x,y)}{p(x)p(y)}\n",
        "\\end{equation*}\n",
        "where $p(x)$ and $p(y)$ are the probabilities of occurrence of events $x$ and $y$, and $p(x,y)$ is the probability of co-occurrence of $x$ and $y$.\n",
        "\n",
        "For this assignment, the \"events\" that we are interested in are occurrences of tokens on lines of text in the input file.   For example, one event\n",
        "might represent the occurence of the token \"fire\" a line of text, and another might represent the occurrence of the token \"peace\".   In that case, $p(fire)$ represents the probability that \"fire\" will occur on a line of text, and $p(fire,peace)$ represents the probability that *both* \"fire\" and \"peace\" will occur on the *same* line.   For the purposes of these PMI computations, it does not matter how many times a given token occures on a single line.   Either a line contains a particular token (at least once), or it does not.   For example, consider this line of text:\n",
        "\n",
        "> three three three, said thrice\n",
        "\n",
        "For this line, the following token-pair events have occurred:\n",
        "- (three, said)\n",
        "- (three, thrice)\n",
        "- (said, three)\n",
        "- (said, thrice)\n",
        "- (thrice, three)\n",
        "- (thrice, said)\n",
        "\n",
        "Note that we are not interested in \"reflexive\" pairs, such as (thrice,thrice).\n",
        "\n",
        "In addition to the probabilities of events, we will also be interested in the absolute *number* of occurences of particular events, e.g., the number of lines in which \"fire\" occurs.   We will use $n(x)$ to represent the these numbers.\n",
        "\n",
        "Your main task for this assignment is to write Python code to analyze the PMI of tokens from Shakespeare's plays.    Based this analysis, we want to be able to answer two types of queries:\n",
        "\n",
        "* Two-Token Queries: Given a pair of tokens, $x$ and $y$, report the number of lines on which that pair co-occurs ($n(x,y)$) as well as $PMI(x,y)$.\n",
        "* One-Token Queries: Given a single token, $x$, report the number of lines on which that token occurs ($n(x)$).   In addition, report the five tokens that have the largest PMI with respect to $x$ (and their PMIs).   That is, report the five $y$'s for which $PMI(x,y)$ is largest.\n",
        "\n",
        "To avoid reporting spurious results for the one-token queries, we are only interested in token pairs that co-occur a sufficient number of times.   Therefore, we will use a *threshold* parameter for one-token queries.   A one-token query should only report pairs of tokens that co-occur at least *threshold* times in the input.   For example, given the threshold 12, a one-token query for \"fire\" the should report the five tokens that have the largest PMI (with respect to \"fire\") among all tokens that co-occur with \"fire\" on at least 12 lines.   If there are fewer than five such tokens, report fewer than five.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBJ1H1i_ENGE"
      },
      "source": [
        "Run the next block to download the text file (`Shakespeare.txt`) and the Python tokenizer module (`simple_tokenize.py`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "rcXDTtuqENqF"
      },
      "outputs": [],
      "source": [
        "!wget -q https://student.cs.uwaterloo.ca/~cs451/content/cs431/Shakespeare.txt\n",
        "!wget -q https://student.cs.uwaterloo.ca/~cs451/content/cs431/simple_tokenize.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVFO1bonNfJo"
      },
      "source": [
        "---\n",
        "#### Question 1  (2/10 marks):\n",
        "\n",
        "Before writing code to handle the PMI queries, start writing some code to answer some simpler questions that give an\n",
        "idea of how big the PMI analysis problem will be.   The box below contains some starter code that reads in the 'Shakespeare.txt' file and tokenizes it one line at time.   (This is the same code you started with for Assignment 0.)  Extend this code to determine (a) the number of *distinct* tokens that exist in 'Shakespeare.txt', and (b) the number of\n",
        "*distinct* token pairs that exist in 'Shakespeare.txt'.  For the purposes of this question, consider the token pair $x,y$ to be distinct from the pair $y,x$, i.e., count them both.   Ignore token pairs of the form $x,x$. Note that we're considering things one line at a time, so the number of unique pairs is NOT (n)(n-1) where n is the number of unique tokens! If a line is \"this is fine\" then there are 6 distinct pairs on that line.\n",
        "\n",
        "As a sanity check, when rounded you should be getting ~26K distinct tokens and ~2M distinct pairs. (Please remember I ROUNDED so e..g if you say \"I got 1.9M is that OK\" then I will worry about you)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": true,
        "id": "AWP7tAfsNfJp"
      },
      "outputs": [],
      "source": [
        "# this imports the SimpleTokenize function from the simple_tokenize.py file that you uploaded\n",
        "from simple_tokenize import simple_tokenize\n",
        "from collections import Counter\n",
        "from itertools import permutations\n",
        "\n",
        "# Now, let's tokenize Shakespeare's plays\n",
        "def distinct_tokens(filename= 'Shakespeare.txt'):\n",
        "  \"\"\"\n",
        "  The function takes in a txt file and returns None. It prints out the total number of\n",
        "  distinct tokens and total number of distinct token pairs.\n",
        "  \"\"\"\n",
        "  word_count = Counter()\n",
        "  token_pairs = set() # a set of token pairs\n",
        "  with open(filename) as f:\n",
        "    for line in f:\n",
        "      t = simple_tokenize(line)\n",
        "      word_count.update(t)\n",
        "      token_pairs.update(\n",
        "          set((x,y) for x,y in permutations(t,2) if x != y)\n",
        "          )\n",
        "  print(\"Total distinct tokens: \", len(word_count))\n",
        "  print(\"Total distinct token pairs: \", len(token_pairs))\n",
        "\n",
        "# extend this code to answer Question 1.\n",
        "# when your code is executed, it should print the number of distinct tokens and the number of distinct token pairs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "distinct_tokens()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIuPXbS5WonS",
        "outputId": "5ac9a643-520f-4332-c449-399210b5f3ec"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total distinct tokens:  25975\n",
            "Total distinct token pairs:  1969760\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpFi9CxkNfJq"
      },
      "source": [
        "---\n",
        "\n",
        "#### Question 2 (6/10 marks):\n",
        "Next, write Python code to answer the one-token and two-token queries described above, for 'Shakespeare.txt'.   The code cell below contains some starter code that implements a simple text-based query interface.  It allows a user to ask a series of one-token or two-token queries.   Try running the starter code to get a sense of how the interface behaves.    \n",
        "\n",
        "Your task is to write code to read and tokenize 'Shakespeare.txt', record information that will allow both types of PMI queries to be answered, and then answer queries that are posed through the query interface.  Make sure that your code is well commented, so that it will be clear to the markers.\n",
        "\n",
        "If you cannot get answers to both types of queries working, try to get at least one type working, for partial credit.\n",
        "\n",
        "Please appropriately handle cases where the token being queried isn't in the file. Print an appropriate error message and continue reading input.\n",
        "(We won't mark the text of your error messages, but it's bad practice to crash if someone has a typo)\n",
        "\n",
        "Sanity checks:\n",
        "\n",
        "```\n",
        "Input 1 or 2 space-separated tokens (return to quit): the end\n",
        "n(the,end) = 157\n",
        "PMI(the,end) = 0.3505058356267105\n",
        "```\n",
        "\n",
        "If you got n=147 then you didn't use `simple_tokenize()`, you used `split()` and were thrown off by puntuation and capitlization.\n",
        "\n",
        "(If you haven't been told this before, floating point numbers are inexact. You do not need to get exactly those digits, but should match at least 4 of them! Make sure you used log base 10)\n",
        "\n",
        "If your PMI is 2.9ish then you probably are counting how many times each token occurrs rather than how many LINES contain that token.\n",
        "\n",
        "Because of the phrase \"at rope's end\", you should also expect that \"rope's\" doesn't really show up outside this phrase, so it will have a high PMI with \"end\".\n",
        "\n",
        "```\n",
        "n(end,rope's) = 5, PMI(end,rope's) = 2.5ish\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# just to test the number of words\n",
        "\n",
        "with open('Shakespeare.txt') as f:\n",
        "  count = 0\n",
        "  for line in f:\n",
        "    t = simple_tokenize(line)\n",
        "    if \"the\" in t:\n",
        "      count += 1\n",
        "print(count)"
      ],
      "metadata": {
        "id": "Tagvw33-5tv4",
        "outputId": "22565e3a-44f8-4e9f-ceb4-7b056229ceb7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "3of7ltFENfJr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "outputId": "b27e8479-5b17-446a-ad93-4b21e5da70a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input 1 or 2 space-separated tokens (return to quit): abc\n",
            "Input a positive integer frequency threshold: 10\n",
            "  n(abc) = 0\n",
            "  high PMI tokens with respect to abc (threshold: 10):\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "list index out of range",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2946979974.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"  n({0}) = {1}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_line_count\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mq_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"  high PMI tokens with respect to {0} (threshold: {1}):\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"    n({0},{1}) = {2},  PMI({0},{1}) = {3}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstripe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mq_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"    n({0},{1}) = {2},  PMI({0},{1}) = {3}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstripe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mq_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"    n({0},{1}) = {2},  PMI({0},{1}) = {3}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstripe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mq_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ],
      "source": [
        "# this imports the SimpleTokenize function from the simple_tokenize.py file that you uploaded\n",
        "from simple_tokenize import simple_tokenize\n",
        "# the log function for computing PMI\n",
        "# for the sake of consistency across solutions, please use log base 10\n",
        "from math import log\n",
        "from collections import defaultdict\n",
        "\n",
        "###################################################################################################################\n",
        "#  replace this with your PMI analysis code, so that you can support the user interface below\n",
        "#  it should read and tokenize Shakespeare.txt, and store enough information in Python data structures\n",
        "#  to allow you to answer the PMI queries below\n",
        "###################################################################################################################\n",
        "\n",
        "token_line_count = defaultdict(int) # keeps track of all n(x) or n(y)\n",
        "stripe = defaultdict(lambda: defaultdict(int)) # keeps track of all n(x,y)\n",
        "total_lines = 0\n",
        "# open the text file and tokenize each line\n",
        "with open('Shakespeare.txt') as f:\n",
        "  for line in f:\n",
        "    total_lines += 1\n",
        "    t = simple_tokenize(line)\n",
        "    for token in t:\n",
        "      token_line_count[token] += 1\n",
        "\n",
        "    tokens_list = list(set(t)) # gets rid of duplicates\n",
        "    for pos1 in range(len(tokens_list)):\n",
        "      for pos2 in range(pos1+1, len(tokens_list)):\n",
        "        word1, word2 = tokens_list[pos1], tokens_list[pos2]\n",
        "        stripe[word1][word2] += 1\n",
        "        stripe[word2][word1] += 1\n",
        "\n",
        "def findPMI(x, y):\n",
        "  \"\"\"\n",
        "  The function findPMI takes in a string, integers and dictionaries and returns a float\n",
        "  representing the PMI of words x and y. The function takes values:\n",
        "  x: the word of interest\n",
        "  x_count: the number of lines word x occurs\n",
        "  totalLines: the total lines in the text file\n",
        "  cooccur_dict: a dictionary with co-occurring words, y. The values are the\n",
        "    number of lines on which y appear\n",
        "  y_dict: a dictionary of unique words in the text file and value representing\n",
        "    the number of lines on which y appears\n",
        "\n",
        "  findPMI: Str Int Dict Dict Int-> Dict of Strs to Ints\n",
        "  \"\"\"\n",
        "  n_x = token_line_count[x]\n",
        "  n_y = token_line_count[y]\n",
        "  p_x = n_x/total_lines\n",
        "  p_y = n_y/total_lines\n",
        "  p_xy = stripe[x][y]/total_lines\n",
        "  return log(p_xy/(p_x*p_y), 10)\n",
        "\n",
        "###################################################################################################################\n",
        "#  the user interface below defines the types of PMI queries that users can ask\n",
        "#  you will need to modify it - where indicated - to access the results of your PMI analysis (above)\n",
        "#  so that the queries can be answered\n",
        "###################################################################################################################\n",
        "\n",
        "while True:\n",
        "    q = input(\"Input 1 or 2 space-separated tokens (return to quit): \")\n",
        "    if len(q) == 0:\n",
        "        break\n",
        "    q_tokens = simple_tokenize(q)\n",
        "    if len(q_tokens) == 1:\n",
        "        threshold = 0\n",
        "        while threshold <= 0:\n",
        "            try:\n",
        "                threshold = int(input(\"Input a positive integer frequency threshold: \"))\n",
        "            except ValueError:\n",
        "                print(\"Threshold must be a positive integer!\")\n",
        "                continue\n",
        "\n",
        "        # Put code here to answer a One-Token Query with token q_tokens[0] and the specified threshold,\n",
        "        # and output the result.\n",
        "        results = []\n",
        "        for key in stripe[q_tokens[0]]: # looking into the inner dictionary\n",
        "          if stripe[q_tokens[0]][key] >= threshold:\n",
        "            pmi = findPMI(q_tokens[0], key)\n",
        "            results.append((key, pmi))\n",
        "        results.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # The print() statements below exist to show you the desired output format.\n",
        "        # Replace them with your own output code, which should produce results in a similar format.\n",
        "        print(\"  n({0}) = {1}\".format(q_tokens[0], token_line_count[q_tokens[0]]))\n",
        "        print(\"  high PMI tokens with respect to {0} (threshold: {1}):\".format(q_tokens[0],threshold))\n",
        "        print(\"    n({0},{1}) = {2},  PMI({0},{1}) = {3}\".format(q_tokens[0], results[0][0], stripe[q_tokens[0]][results[0][0]], results[0][1]))\n",
        "        print(\"    n({0},{1}) = {2},  PMI({0},{1}) = {3}\".format(q_tokens[0], results[1][0], stripe[q_tokens[0]][results[1][0]], results[1][1]))\n",
        "        print(\"    n({0},{1}) = {2},  PMI({0},{1}) = {3}\".format(q_tokens[0], results[2][0], stripe[q_tokens[0]][results[2][0]], results[2][1]))\n",
        "        print(\"    n({0},{1}) = {2},  PMI({0},{1}) = {3}\".format(q_tokens[0], results[3][0], stripe[q_tokens[0]][results[3][0]], results[3][1]))\n",
        "        print(\"    n({0},{1}) = {2},  PMI({0},{1}) = {3}\".format(q_tokens[0], results[4][0], stripe[q_tokens[0]][results[4][0]], results[4][1]))\n",
        "        # in the above, all XXX values should be at least as large as the threshold\n",
        "\n",
        "    elif len(q_tokens) == 2:\n",
        "        # Put code here to answer a Two-Token Query with tokens q_tokens[0] and q_tokens[1]\n",
        "        # As was the case for the One-Token query, the print statements below show the desired output format\n",
        "        # Replace them with your own output code\n",
        "\n",
        "        # we initialize a variable n_count2 to keep track of how many lines\n",
        "        # the token pair (q_tokens[0],q_tokens[1]) is found in\n",
        "        n_count2 = 0\n",
        "\n",
        "        # open the text file and tokenize each line\n",
        "\n",
        "\n",
        "        print(\"  n({0},{1}) = {2}\".format(q_tokens[0],q_tokens[1], n_count2))\n",
        "        print(\"  PMI({0},{1}) = Y.YYY\".format(q_tokens[0],q_tokens[1]))\n",
        "    else:\n",
        "        print(\"Input must consist of 1 or 2 space-separated tokens!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zjE_iW6NfJt"
      },
      "source": [
        "---\n",
        "\n",
        "#### Question 3 (2/10 marks):\n",
        "\n",
        "Suppose that you try to run your PMI analysis on larger files:  say, 10 times, or 100 times, or 1000 times larger than 'Shakespeare.txt'.    As the input file grows larger, what will happen to the execution of your program?   Will it get slower?   How much slower?   Will it eventually fail to run?   If so, why?\n",
        "\n",
        "Consider two seperate parts: The initial processing of the file, and then the time required to run the queries themselves.\n",
        "(If you did not do any initial processing and reloaded everything for each query, then please go fix that, that's not okay.)\n",
        "\n",
        "\n",
        "In the cell below, briefly (one or two paragraphs), discuss what will happen if the input to your analysis grows.  We're not looking for an exact or empirical analysis of the behaviour of your program as a function of input size.  Rather, we are looking for a discussion of trends and limits.\n",
        "\n",
        "(Tip since we probably haven't got that far in lectures yet: Vocabulary size for a document actually grows unbounded, proportional to the log of the document size!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SVhvS1tNfJu"
      },
      "source": [
        "#### Answer to Question 3:\n",
        "\n",
        "As the input size grows, the program will execute slower for both the processing of the file and execution of the program. However, the impact is different for the two cases.\n",
        "\n",
        "As larger files are inputted, the processing time increases significantly as the different permutataions of words have to be verified for co-occurences at each line and the vocabulary size increases unbounded proportional to the log(document size). Hence, for a significantly larger file, the program will fail as it will run out of memory.\n",
        "\n",
        "For query execution, this will remain fast as it will rely on returning precomputed results. However, since the vocabulary grows proportionally to the log(document size) there could be memory bloat. The procomputed data becomes too large and there is insufficient memory for query execution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vsr1IWaDNfJu"
      },
      "source": [
        "---\n",
        "Don't forget to save your workbook!   (It's a good idea to do this regularly, while you are working.)   When you are finished and you are ready to submit your assignment, download your notebook file (.ipynb) from the hub to your machine, and then follow the submission instructions in the assignment."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}