{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dlkt101101/CS431/blob/main/A6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "id": "I1gm9-t0i7Ok",
        "tags": [
          "instructions"
        ]
      },
      "source": [
        "## CS431/631 Data Intensive Distributed Computing\n",
        "### Winter 2025 - Assignment 6\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "id": "-CB3G_gbi7Ok",
        "tags": [
          "instructions"
        ]
      },
      "source": [
        "**Please edit this (text) cell to provide your name and UW student ID number!**\n",
        "* **Name:** _Darren Lam Kin Teng_\n",
        "* **ID:** _replace this with your UW student ID number_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5BLErorljlr"
      },
      "source": [
        "Let's first install Spark. This will take a minute to finish."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lh5NCoc8fsSO"
      },
      "outputs": [],
      "source": [
        "!apt-get update -qq > /dev/null\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!curl -Os https://student.cs.uwaterloo.ca/~cs451/spark/spark-3.4.3-bin-hadoop3.tgz\n",
        "!tar xzf spark-3.4.3-bin-hadoop3.tgz\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOjh7W7a2thx"
      },
      "source": [
        "On Colab: set the environment variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJRCcNdY2thx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.3-bin-hadoop3\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll need to download the data we'll be working with."
      ],
      "metadata": {
        "id": "Lgsv8CzH3A3z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://student.cs.uwaterloo.ca/~cs451/assignments/data/sensor_readings_24.csv"
      ],
      "metadata": {
        "id": "3mKygf8r3Rv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll also need to create a local server that will stream this data to us. (We \"could\" stream directly from the file, but then all of the data arrives at once and that's not our goal)."
      ],
      "metadata": {
        "id": "6RKf77TT3aCX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import socket\n",
        "import time\n",
        "import threading\n",
        "\n",
        "try:\n",
        "  stopEvent.set() # if there's an old thread running, stop it\n",
        "  time.sleep(3)\n",
        "except:\n",
        "  pass\n",
        "\n",
        "def handle_new_client(client_socket,addr):\n",
        "    try:\n",
        "        # Load data\n",
        "        File = open('sensor_readings_24.csv', \"r\")\n",
        "        lines = File.read()\n",
        "\n",
        "        for line in lines.split(\"\\n\"):\n",
        "            client_socket.send((line+'\\n').encode())\n",
        "            time.sleep(0.1)\n",
        "    except BrokenPipeError:\n",
        "      pass # this just means the client disconnected abruptly, which it does\n",
        "    finally:\n",
        "        client_socket.close()\n",
        "\n",
        "def handleServer(stopEvent):\n",
        "  server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "  server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n",
        "  server_socket.bind(('localhost', 4321))  # local server-client\n",
        "  server_socket.listen(3) # Max. 3 connections\n",
        "  try:\n",
        "    while not stopEvent.is_set():\n",
        "        try:\n",
        "            client_socket, addr = server_socket.accept()\n",
        "            workerThread = threading.Thread(target=handle_new_client, args=(client_socket, addr))\n",
        "            workerThread.start()\n",
        "        except Exception as e:\n",
        "          print(e)\n",
        "  finally:\n",
        "    server_socket.close()\n",
        "\n",
        "stopEvent = threading.Event()\n",
        "serverThread = threading.Thread(target=handleServer, args=(stopEvent,))\n",
        "serverThread.start()"
      ],
      "metadata": {
        "id": "v-tg240O3kNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILheUROOhprv"
      },
      "source": [
        "Now that you installed Spark and Java in Colab, it is time to set the environment path which enables you to run Pyspark in your Colab environment. Next we define a function that helps us create SparkContext and StreamingContext. Unlike prior assignments, this needs to be a function as we will need to call it each time we want to stream - the old streaming context must be ended and disposed of, and a new one created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xnxe-BhPmbBW"
      },
      "outputs": [],
      "source": [
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.streaming import StreamingContext\n",
        "import time\n",
        "\n",
        "# This function creates SparkContext and StreamingContext\n",
        "# Do not change this function\n",
        "def initStreamingContext():\n",
        "    try:\n",
        "      ssc.end()\n",
        "    except:\n",
        "      pass\n",
        "    finally:\n",
        "      spark_conf = SparkConf()\\\n",
        "            .setAppName(\"YourTest\")\\\n",
        "            .setMaster(\"local[*]\")\n",
        "      sc = SparkContext.getOrCreate(spark_conf)\n",
        "      # Creating Streaming Context with batch window size of 1 second\n",
        "      ssc = StreamingContext(sc, 1)\n",
        "      return ssc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeHmOcUv4hg3"
      },
      "source": [
        "### Overview\n",
        "\n",
        "The data we use in this assignment was collected from the sensors installed on a wall-navigating robot. The robot uses 24 ultrasound sensors arranged circularly around its \"waist\". The numbering of the ultrasound sensors starts at the front of the robot and increases in clockwise direction. To make our data streaming scenario realistic, we have developed a server that streams the robot's data to your program (as if you are really getting the data live from the robot). You will use Spark Streaming to perform a few simple tasks on this data.\n",
        "\n",
        "Every line of data transmitted by the server corresponds to a measurement done by the robot. Here is one line of such data:\n",
        "\n",
        "```\n",
        "0.438,0.498,3.625,3.645,5.000,2.918,5.000,2.351,2.332,2.643,1.698,1.687,1.698,1.717,1.744,0.593,0.502,0.493,0.504,0.445,0.431,0.444,0.440,0.429,Slight-Right-Turn\n",
        "```\n",
        "The raw values are the measurements of all 24 ultrasound sensors and the corresponding movement type which can be one of the following:\n",
        "Move-Forward, Slight-Right-Turn, Sharp-Right-Turn, and Slight-Left-Turn.\n",
        "\n",
        "Run the following block to see the flow of data for 5 seconds. This is normal that in each run the data is slightly shifted in time because it depends on the delay of receiving the data from the server across the Internet.Therefore, in every 1 second batch, we might have different numbers of measurements and it can vary across different runs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGFcbRKL7ICo"
      },
      "outputs": [],
      "source": [
        "# Let's create ssc.\n",
        "ssc = initStreamingContext()\n",
        "# We initialize a DStream by connecting it to a TCP socket.\n",
        "# The server will start sending data which goes to the robotData DStream.\n",
        "robotData = ssc.socketTextStream(\"localhost\", 4321)\n",
        "robotData.pprint()\n",
        "ssc.start()\n",
        "# Just wait 5 seconds before we stop the stream.\n",
        "time.sleep(5)\n",
        "ssc.stop()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I61flWvt8FLp"
      },
      "source": [
        "### Question 1 (4/10 marks)\n",
        "An important factor for a navigating robot is avoiding obstacles. This is why there are so many sensors on this robot to measure the distance to all surrounding obstacles in all directions. Write a program that every second reports the smallest distances measured in the last 3 seconds by any sensor.\n",
        "\n",
        "\n",
        "For example, if the robot performs the following two measurements in the last 3 seconds:\n",
        "```\n",
        "0.482,0.512,0.524,3.665,2.953,2.940,2.940,2.629,1.709,2.311,1.660,1.640,1.635,1.654,1.755,0.563,0.545,0.475,0.475,0.485,0.464,0.459,0.468,0.478,Slight-Right-Turn\n",
        "0.484,0.514,0.525,3.667,2.954,2.938,2.941,2.957,1.707,2.310,1.658,1.638,1.633,1.652,1.753,0.682,0.535,0.475,0.475,0.544,0.465,0.457,0.469,0.483,Slight-Right-Turn\n",
        "```\n",
        "your program must print:\n",
        "```\n",
        "-------------------------------------------\n",
        "Time: 2020-11-27 23:56:24\n",
        "-------------------------------------------\n",
        "0.457\n",
        "```\n",
        "Note that this is the output for one 3-second window. The program should keep printing the smallest distance for all windows as long as your program is running.\n",
        "\n",
        "All of the calculations must be performed in Spark and not the driver program. You must use `pprint` at the end to print the results.\n",
        "\n",
        "You can consult [this document](https://spark.apache.org/docs/2.2.0/streaming-programming-guide.html) to find more about available transformations on DStreams.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7259HnozDivW"
      },
      "outputs": [],
      "source": [
        "ssc = initStreamingContext()\n",
        "robotData = ssc.socketTextStream(\"localhost\", 4321)\n",
        "#### Your Solution to Question 1 here\n",
        "\n",
        "\n",
        "ssc.start()\n",
        "# Let's wait for 10 seconds before we stop the program.\n",
        "# Feel free to change this value but make sure you change it back to 10 before submission.\n",
        "time.sleep(10)\n",
        "ssc.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwkvRCWmEPZH"
      },
      "source": [
        "### Question 2 (6/10 marks)\n",
        "In this question, you characterize movements of the robot. The last field in every line indicates the movement type. Write a program that every second reports what movements were performed by the robot in the last 3 seconds. You should also report the ratio of each movement. For example, if 10 movements are \"Slight-Right-Turn\" out of 50 movements in the last 3 seconds, your program should print: Slight-Right-Turn 0.2. Finally, the movements should be reported in the descending order of the radios. Make sure you print \"----------\" to indicate the end of each window.\n",
        "\n",
        "\n",
        "Here is an example of the expected output:\n",
        "```\n",
        "Slight-Right-Turn 0.6666666666666666\n",
        "Sharp-Right-Turn 0.3333333333333333\n",
        "----------\n",
        "Sharp-Right-Turn 0.5384615384615384\n",
        "Slight-Right-Turn 0.46153846153846156\n",
        "----------\n",
        "Slight-Right-Turn 0.6590909090909091\n",
        "Sharp-Right-Turn 0.3409090909090909\n",
        "----------\n",
        "Slight-Right-Turn 0.75\n",
        "Sharp-Right-Turn 0.19642857142857142\n",
        "Move-Forward 0.05357142857142857\n",
        "----------\n",
        "```\n",
        "Note that all of these calculations much be performed in Spark and not the driver program. The driver program should only print the final result.\n",
        "\n",
        "**Hint**: since this question asks you to print results with a custom format you cannot use pprint(). Instead, prepare the results using some transformations and at the end use forEachRDD() to collect and print the results. Please look up forEachRDD in the API to learn how it works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zq5XvBNGIFN"
      },
      "outputs": [],
      "source": [
        "ssc = initStreamingContext()\n",
        "robotData = ssc.socketTextStream(\"localhost\", 4321)\n",
        "#### Your Solution to Question 2 here\n",
        "\n",
        "ssc.start()\n",
        "# Let's wait for 10 seconds before we stop the program.\n",
        "# Feel free to change this value but make sure you change it back to 10 before submission.\n",
        "time.sleep(10)\n",
        "ssc.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "id": "3hVKBrDDi7Oo",
        "tags": [
          "instructions"
        ]
      },
      "source": [
        "---\n",
        "Don't forget to save your workbook!   When you are finished and you are ready to submit your assignment, download your notebook file (.ipynb) from the hub to your machine, and then follow the submission instructions in the assignment."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}